{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLLXilK6t0JmG3e1uzTUuQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sidhtang/ai-education-companion/blob/main/ai_educational_companion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install google-generativeai gradio numpy tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yE9VT62QwF-w",
        "outputId": "1595d1d2-403a-4a70-d1ec-c1378219cbcf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.29.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.169.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.13.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.10.0 (from gradio)\n",
            "  Downloading gradio_client-1.10.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.17)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading gradio-5.29.0-py3-none-any.whl (54.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.9/322.9 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.29.0 gradio-client-1.10.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.8 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVobkM6bxfer",
        "outputId": "49c657ae-e19d-4bcf-df50-ceb82ec42c94"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dotenv\n",
            "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
            "Collecting python-dotenv (from dotenv)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv, dotenv\n",
            "Successfully installed dotenv-0.9.9 python-dotenv-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque, Counter\n",
        "import random\n",
        "from dotenv import load_dotenv\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[logging.FileHandler(\"learning_companion.log\"), logging.StreamHandler()]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Load environment variables (for API keys)\n",
        "load_dotenv()\n",
        "\n",
        "# =============== 1. ENHANCED LLM INTEGRATION ===============\n",
        "\n",
        "class ContentGenerator:\n",
        "    \"\"\"Advanced content generation with multiple LLM options and caching\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.setup_gemini()\n",
        "        self.content_cache = {}\n",
        "        self.fallback_enabled = True\n",
        "\n",
        "    def setup_gemini(self):\n",
        "        \"\"\"Setup Gemini API with safe error handling\"\"\"\n",
        "        try:\n",
        "            api_key = os.getenv(\"GEMINI_API_KEY\") or \"\"\n",
        "            genai.configure(api_key=api_key)\n",
        "            self.gemini_model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "            logger.info(\"Gemini API initialized successfully\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to initialize Gemini API: {str(e)}\")\n",
        "            self.gemini_model = None\n",
        "\n",
        "    def _get_cache_key(self, topic, difficulty, style, content_type):\n",
        "        \"\"\"Create a unique cache key\"\"\"\n",
        "        return f\"{topic}|{difficulty}|{style}|{content_type}\"\n",
        "\n",
        "    def generate_content(self, topic, difficulty, style, content_type=\"full\"):\n",
        "        \"\"\"Generate educational content with caching and improved prompting\"\"\"\n",
        "        # Check cache first\n",
        "        cache_key = self._get_cache_key(topic, difficulty, style, content_type)\n",
        "        if cache_key in self.content_cache:\n",
        "            logger.info(f\"Cache hit for {cache_key}\")\n",
        "            return self.content_cache[cache_key]\n",
        "\n",
        "        # Style-specific instructions\n",
        "        style_instructions = {\n",
        "            \"visual\": \"Include diagrams, charts, and visual representations. Use descriptive language that evokes mental imagery. Format key concepts in tables when appropriate.\",\n",
        "            \"auditory\": \"Structure content as a conversational dialogue. Include mnemonics, rhythmic patterns, and verbal cues. Emphasize verbal explanations.\",\n",
        "            \"kinesthetic\": \"Design hands-on experiments or activities. Include interactive elements where students can apply concepts immediately. Suggest real-world applications.\",\n",
        "            \"reading/writing\": \"Provide well-structured text with clear headings. Include detailed written explanations and opportunities for note-taking.\",\n",
        "            \"multimodal\": \"Blend multiple learning modalities. Combine visual elements with verbal explanations and suggested activities.\"\n",
        "        }\n",
        "\n",
        "        # Difficulty-specific adaptations\n",
        "        complexity_adjustments = {\n",
        "            \"beginner\": \"Use simple language and basic concepts. Avoid jargon and explain all technical terms. Focus on foundational knowledge.\",\n",
        "            \"intermediate\": \"Build on basic concepts with moderate complexity. Introduce more technical vocabulary. Connect concepts to create a broader understanding.\",\n",
        "            \"advanced\": \"Use sophisticated language and complex concepts. Explore nuanced aspects of the topic. Challenge students with deeper analysis.\"\n",
        "        }\n",
        "\n",
        "        # Content type specific format\n",
        "        format_instructions = {\n",
        "            \"full\": \"\"\"\n",
        "Structure your response in these clear sections:\n",
        "1. CONCEPT EXPLANATION: Provide a clear, concise explanation of the core concept.\n",
        "2. KEY POINTS: List 3-5 essential takeaways in bullet points.\n",
        "3. DETAILED EXAMPLE: Walk through a comprehensive example step-by-step.\n",
        "4. PRACTICE PROBLEM: Create a challenging but appropriate practice problem.\n",
        "5. SOLUTION: Provide a detailed solution with explanation.\n",
        "6. FURTHER EXPLORATION: Suggest ways to deepen understanding of this topic.\n",
        "            \"\"\",\n",
        "            \"lesson\": \"\"\"\n",
        "Structure your response in these sections:\n",
        "1. CONCEPT EXPLANATION: Provide a clear, concise explanation of the core concept.\n",
        "2. KEY POINTS: List 3-5 essential takeaways in bullet points.\n",
        "3. DETAILED EXAMPLE: Walk through a comprehensive example step-by-step.\n",
        "            \"\"\",\n",
        "            \"quiz\": \"\"\"\n",
        "Create exactly one practice problem related to this topic, appropriate for the specified difficulty level.\n",
        "Then provide a detailed solution with explanation, showing all steps clearly.\n",
        "Label the sections as:\n",
        "- PRACTICE PROBLEM\n",
        "- SOLUTION\n",
        "            \"\"\"\n",
        "        }\n",
        "\n",
        "        # Generate enhanced prompt\n",
        "        prompt = f\"\"\"\n",
        "You are an expert educational content creator specializing in personalized learning.\n",
        "\n",
        "TOPIC: {topic}\n",
        "DIFFICULTY: {difficulty}\n",
        "LEARNING STYLE: {style}\n",
        "\n",
        "SPECIFIC STYLE GUIDANCE: {style_instructions.get(style, style_instructions[\"multimodal\"])}\n",
        "DIFFICULTY ADAPTATION: {complexity_adjustments.get(difficulty, complexity_adjustments[\"intermediate\"])}\n",
        "\n",
        "{format_instructions.get(content_type, format_instructions[\"full\"])}\n",
        "\n",
        "Make all content highly engaging and relevant to real-world applications when possible.\n",
        "        \"\"\"\n",
        "\n",
        "        # Generate content with error handling\n",
        "        try:\n",
        "            if self.gemini_model:\n",
        "                response = self.gemini_model.generate_content(prompt)\n",
        "                content = response.text\n",
        "                # Cache the result\n",
        "                self.content_cache[cache_key] = content\n",
        "                return content\n",
        "            else:\n",
        "                return self._generate_fallback_content(topic, difficulty, style, content_type)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating content: {str(e)}\")\n",
        "            return self._generate_fallback_content(topic, difficulty, style, content_type)\n",
        "\n",
        "    def _generate_fallback_content(self, topic, difficulty, style, content_type):\n",
        "        \"\"\"Generate fallback content when API fails\"\"\"\n",
        "        if content_type == \"quiz\":\n",
        "            return f\"\"\"\n",
        "PRACTICE PROBLEM:\n",
        "Create a brief explanation of {topic} at a {difficulty} level, considering {style} learning preferences.\n",
        "\n",
        "SOLUTION:\n",
        "A good explanation would include key concepts, examples, and applications relevant to {topic}.\n",
        "            \"\"\"\n",
        "        else:\n",
        "            return f\"\"\"\n",
        "# Learning Content: {topic}\n",
        "\n",
        "Sorry, I'm currently having trouble connecting to the content generation service.\n",
        "\n",
        "Please try again later or explore these resources about {topic}:\n",
        "- Check educational websites like Khan Academy or Coursera\n",
        "- Look for {topic} in textbooks or academic journals\n",
        "- Try searching for \"{topic} tutorial\" online\n",
        "\n",
        "This is a temporary technical issue, and we appreciate your patience.\n",
        "            \"\"\"\n",
        "\n",
        "# =============== 2. ADVANCED REINFORCEMENT LEARNING SYSTEM ===============\n",
        "\n",
        "class LearningState:\n",
        "    \"\"\"Comprehensive student learning state representation\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.features = {\n",
        "            # Performance metrics\n",
        "            'correct_answers': 0,\n",
        "            'incorrect_answers': 0,\n",
        "            'consecutive_correct': 0,\n",
        "            'total_attempts': 0,\n",
        "            'avg_response_time': 0,\n",
        "\n",
        "            # Current settings\n",
        "            'difficulty_level': 0,  # 0=beginner, 1=intermediate, 2=advanced\n",
        "            'learning_style': 0,    # 0=visual, 1=auditory, 2=kinesthetic, 3=reading/writing, 4=multimodal\n",
        "\n",
        "            # Engagement metrics\n",
        "            'session_duration': 0,\n",
        "            'topics_explored': 0,\n",
        "\n",
        "            # Temporal features\n",
        "            'time_since_last_activity': 0,\n",
        "            'day_of_week': datetime.now().weekday(),\n",
        "            'time_of_day': datetime.now().hour,\n",
        "        }\n",
        "\n",
        "        self.difficulty_map = ['beginner', 'intermediate', 'advanced']\n",
        "        self.style_map = ['visual', 'auditory', 'kinesthetic', 'reading/writing', 'multimodal']\n",
        "        self.history = []\n",
        "        self.topic_history = Counter()\n",
        "        self.last_activity_time = datetime.now()\n",
        "\n",
        "    def update(self, correct, response_time, current_topic):\n",
        "        \"\"\"Update state based on student interaction\"\"\"\n",
        "        # Record previous state for history\n",
        "        self.history.append(self.features.copy())\n",
        "\n",
        "        # Update performance metrics\n",
        "        self.features['total_attempts'] += 1\n",
        "        if correct:\n",
        "            self.features['correct_answers'] += 1\n",
        "            self.features['consecutive_correct'] += 1\n",
        "        else:\n",
        "            self.features['incorrect_answers'] += 1\n",
        "            self.features['consecutive_correct'] = 0\n",
        "\n",
        "        # Update response time metrics\n",
        "        prev_avg = self.features['avg_response_time']\n",
        "        n = self.features['total_attempts']\n",
        "        self.features['avg_response_time'] = (prev_avg * (n-1) + response_time) / n\n",
        "\n",
        "        # Update engagement metrics\n",
        "        now = datetime.now()\n",
        "        time_diff = (now - self.last_activity_time).total_seconds()\n",
        "        self.features['time_since_last_activity'] = time_diff\n",
        "        self.last_activity_time = now\n",
        "\n",
        "        # Update session metrics\n",
        "        self.features['session_duration'] += time_diff\n",
        "\n",
        "        # Update topic tracking\n",
        "        if current_topic not in self.topic_history:\n",
        "            self.features['topics_explored'] += 1\n",
        "        self.topic_history[current_topic] += 1\n",
        "\n",
        "        # Update temporal features\n",
        "        self.features['day_of_week'] = now.weekday()\n",
        "        self.features['time_of_day'] = now.hour\n",
        "\n",
        "        return self.features.copy()\n",
        "\n",
        "    def get_current_difficulty(self):\n",
        "        \"\"\"Get current difficulty setting as string\"\"\"\n",
        "        idx = min(int(self.features['difficulty_level']), len(self.difficulty_map)-1)\n",
        "        return self.difficulty_map[idx]\n",
        "\n",
        "    def get_current_style(self):\n",
        "        \"\"\"Get current learning style setting as string\"\"\"\n",
        "        idx = min(int(self.features['learning_style']), len(self.style_map)-1)\n",
        "        return self.style_map[idx]\n",
        "\n",
        "    def set_difficulty(self, difficulty):\n",
        "        \"\"\"Set difficulty by name\"\"\"\n",
        "        if difficulty in self.difficulty_map:\n",
        "            self.features['difficulty_level'] = self.difficulty_map.index(difficulty)\n",
        "        else:\n",
        "            self.features['difficulty_level'] = 0\n",
        "\n",
        "    def set_style(self, style):\n",
        "        \"\"\"Set learning style by name\"\"\"\n",
        "        if style in self.style_map:\n",
        "            self.features['learning_style'] = self.style_map.index(style)\n",
        "        else:\n",
        "            self.features['learning_style'] = 0\n",
        "\n",
        "    def get_state_tensor(self):\n",
        "        \"\"\"Convert state to tensor for RL model\"\"\"\n",
        "        return torch.tensor([list(self.features.values())], dtype=torch.float32)\n",
        "\n",
        "    def get_performance_ratio(self):\n",
        "        \"\"\"Calculate performance ratio for adaptive difficulty\"\"\"\n",
        "        total = self.features['correct_answers'] + self.features['incorrect_answers']\n",
        "        if total == 0:\n",
        "            return 0.5\n",
        "        return self.features['correct_answers'] / total\n",
        "\n",
        "class DQNetwork(nn.Module):\n",
        "    \"\"\"Deep Q-Network with advanced architecture\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(DQNetwork, self).__init__()\n",
        "\n",
        "        # Larger network with dropout for regularization\n",
        "        self.fc1 = nn.Linear(state_size, 128)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, action_size)\n",
        "\n",
        "        # Initialize weights using Xavier initialization\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        nn.init.xavier_uniform_(self.fc3.weight)\n",
        "        nn.init.xavier_uniform_(self.fc4.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        return self.fc4(x)\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"Experience replay buffer with prioritized sampling\"\"\"\n",
        "\n",
        "    def __init__(self, capacity=10000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "        self.priorities = deque(maxlen=capacity)\n",
        "        self.alpha = 0.6  # Priority exponent\n",
        "        self.epsilon = 0.01  # Small constant to avoid zero priority\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done, error=None):\n",
        "        \"\"\"Add experience to buffer with priority\"\"\"\n",
        "        experience = (state, action, reward, next_state, done)\n",
        "\n",
        "        # Set initial priority based on TD error or default to max priority\n",
        "        if error is None:\n",
        "            priority = max(self.priorities) if self.priorities else 1.0\n",
        "        else:\n",
        "            priority = abs(error) + self.epsilon\n",
        "\n",
        "        self.buffer.append(experience)\n",
        "        self.priorities.append(priority)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Sample batch with prioritized experience replay\"\"\"\n",
        "        if len(self.buffer) < batch_size:\n",
        "            batch_size = len(self.buffer)\n",
        "\n",
        "        # Convert priorities to probabilities\n",
        "        priorities = np.array(self.priorities)\n",
        "        probabilities = priorities ** self.alpha\n",
        "        probabilities /= probabilities.sum()\n",
        "\n",
        "        # Sample indices based on priorities\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, p=probabilities)\n",
        "\n",
        "        # Extract experiences\n",
        "        states = torch.cat([self.buffer[i][0] for i in indices])\n",
        "        actions = torch.tensor([self.buffer[i][1] for i in indices], dtype=torch.long)\n",
        "        rewards = torch.tensor([self.buffer[i][2] for i in indices], dtype=torch.float32)\n",
        "        next_states = torch.cat([self.buffer[i][3] for i in indices])\n",
        "        dones = torch.tensor([self.buffer[i][4] for i in indices], dtype=torch.bool)\n",
        "\n",
        "        return states, actions, rewards, next_states, dones, indices\n",
        "\n",
        "    def update_priorities(self, indices, errors):\n",
        "        \"\"\"Update priorities based on new TD errors\"\"\"\n",
        "        for i, error in zip(indices, errors):\n",
        "            if i < len(self.priorities):  # Safety check\n",
        "                self.priorities[i] = abs(error) + self.epsilon\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "class AdaptiveLearningAgent:\n",
        "    \"\"\"Advanced Reinforcement Learning agent for adaptive learning\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # Device configuration\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        logger.info(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Networks\n",
        "        self.policy_net = DQNetwork(state_size, action_size).to(self.device)\n",
        "        self.target_net = DQNetwork(state_size, action_size).to(self.device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()  # Target network in evaluation mode\n",
        "\n",
        "        # Optimizer with learning rate scheduler\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.001)\n",
        "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=100, gamma=0.9)\n",
        "\n",
        "        # Replay buffer\n",
        "        self.memory = ReplayBuffer(capacity=50000)\n",
        "\n",
        "        # Exploration parameters with annealing\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.05\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.gamma = 0.99  # Discount factor\n",
        "\n",
        "        # Training parameters\n",
        "        self.batch_size = 64\n",
        "        self.target_update = 10  # Update target network every N steps\n",
        "        self.steps_done = 0\n",
        "\n",
        "        # Performance tracking\n",
        "        self.rewards_history = []\n",
        "        self.loss_history = []\n",
        "\n",
        "        # Action mapping for interpretability\n",
        "        self.action_map = self._create_action_map()\n",
        "\n",
        "        # Load model if available\n",
        "        self.model_path = \"learning_companion_model.pth\"\n",
        "        self.load_model()\n",
        "\n",
        "    def _create_action_map(self):\n",
        "        \"\"\"Create mapping between action indices and actual changes\"\"\"\n",
        "        actions = []\n",
        "        difficulty_changes = [-1, 0, 1]  # Decrease, maintain, increase\n",
        "        style_changes = [-1, 0, 1]      # Previous, maintain, next style\n",
        "\n",
        "        for d_change in difficulty_changes:\n",
        "            for s_change in style_changes:\n",
        "                actions.append({\n",
        "                    'difficulty_change': d_change,\n",
        "                    'style_change': s_change\n",
        "                })\n",
        "\n",
        "        return actions\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"Select action using epsilon-greedy policy with decaying exploration\"\"\"\n",
        "        if random.random() < self.epsilon:\n",
        "            action = random.randrange(self.action_size)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                state_tensor = state.to(self.device)\n",
        "                q_values = self.policy_net(state_tensor)\n",
        "                action = q_values.max(1)[1].item()\n",
        "\n",
        "        # Decay epsilon\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "        self.steps_done += 1\n",
        "        return action\n",
        "\n",
        "    def apply_action(self, action_idx, learning_state):\n",
        "        \"\"\"Apply selected action to learning state\"\"\"\n",
        "        action = self.action_map[action_idx]\n",
        "\n",
        "        # Get current settings\n",
        "        current_difficulty = learning_state.features['difficulty_level']\n",
        "        current_style = learning_state.features['learning_style']\n",
        "\n",
        "        # Apply difficulty change\n",
        "        new_difficulty = current_difficulty + action['difficulty_change']\n",
        "        new_difficulty = max(0, min(new_difficulty, len(learning_state.difficulty_map) - 1))\n",
        "\n",
        "        # Apply style change\n",
        "        new_style = current_style + action['style_change']\n",
        "        new_style = max(0, min(new_style, len(learning_state.style_map) - 1))\n",
        "\n",
        "        # Update state\n",
        "        learning_state.features['difficulty_level'] = new_difficulty\n",
        "        learning_state.features['learning_style'] = new_style\n",
        "\n",
        "        return learning_state\n",
        "\n",
        "    def calculate_reward(self, correct, consecutive_correct, difficulty_level):\n",
        "        \"\"\"Calculate more nuanced reward based on performance and context\"\"\"\n",
        "        # Base reward for correctness\n",
        "        base_reward = 1.0 if correct else -0.5\n",
        "\n",
        "        # Bonus for consecutive correct answers\n",
        "        streak_bonus = min(consecutive_correct * 0.1, 0.5) if correct else 0\n",
        "\n",
        "        # Difficulty adjustment\n",
        "        difficulty_factor = 1.0 + (difficulty_level * 0.25)\n",
        "\n",
        "        # Combine rewards\n",
        "        reward = (base_reward + streak_bonus) * difficulty_factor\n",
        "\n",
        "        return reward\n",
        "\n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Store transition in replay memory\"\"\"\n",
        "        # Calculate TD error for prioritized replay\n",
        "        with torch.no_grad():\n",
        "            state_tensor = state.to(self.device)\n",
        "            next_state_tensor = next_state.to(self.device)\n",
        "\n",
        "            current_q = self.policy_net(state_tensor)[0][action].item()\n",
        "            next_q = self.target_net(next_state_tensor).max(1)[0].item()\n",
        "\n",
        "            expected_q = reward + self.gamma * next_q * (1 - int(done))\n",
        "            td_error = expected_q - current_q\n",
        "\n",
        "        self.memory.add(state, action, reward, next_state, done, td_error)\n",
        "        self.rewards_history.append(reward)\n",
        "\n",
        "    def optimize_model(self):\n",
        "        \"\"\"Perform one step of optimization\"\"\"\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return 0.0  # Not enough samples\n",
        "\n",
        "        # Sample batch with priorities\n",
        "        states, actions, rewards, next_states, dones, indices = self.memory.sample(self.batch_size)\n",
        "\n",
        "        # Move tensors to device\n",
        "        states = states.to(self.device)\n",
        "        actions = actions.to(self.device)\n",
        "        rewards = rewards.to(self.device)\n",
        "        next_states = next_states.to(self.device)\n",
        "        dones = dones.to(self.device)\n",
        "\n",
        "        # Compute Q values\n",
        "        q_values = self.policy_net(states).gather(1, actions.unsqueeze(1))\n",
        "        next_q_values = self.target_net(next_states).max(1)[0].detach()\n",
        "        expected_q_values = rewards + self.gamma * next_q_values * (~dones)\n",
        "\n",
        "        # Compute TD errors for priority update\n",
        "        td_errors = expected_q_values - q_values.squeeze()\n",
        "\n",
        "        # Huber loss for stability\n",
        "        criterion = nn.SmoothL1Loss()\n",
        "        loss = criterion(q_values.squeeze(), expected_q_values)\n",
        "\n",
        "        # Optimize\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # Gradient clipping to prevent exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Update priorities\n",
        "        self.memory.update_priorities(indices, td_errors.detach().cpu().numpy())\n",
        "\n",
        "        # Update target network\n",
        "        if self.steps_done % self.target_update == 0:\n",
        "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "        # Update learning rate\n",
        "        self.scheduler.step()\n",
        "\n",
        "        # Record loss\n",
        "        loss_value = loss.item()\n",
        "        self.loss_history.append(loss_value)\n",
        "\n",
        "        return loss_value\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"Save model weights\"\"\"\n",
        "        torch.save({\n",
        "            'policy_net': self.policy_net.state_dict(),\n",
        "            'target_net': self.target_net.state_dict(),\n",
        "            'optimizer': self.optimizer.state_dict(),\n",
        "            'scheduler': self.scheduler.state_dict(),\n",
        "            'epsilon': self.epsilon,\n",
        "            'steps_done': self.steps_done,\n",
        "            'rewards_history': self.rewards_history,\n",
        "            'loss_history': self.loss_history\n",
        "        }, self.model_path)\n",
        "        logger.info(f\"Model saved to {self.model_path}\")\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Load model weights if available\"\"\"\n",
        "        try:\n",
        "            if os.path.exists(self.model_path):\n",
        "                checkpoint = torch.load(self.model_path)\n",
        "                self.policy_net.load_state_dict(checkpoint['policy_net'])\n",
        "                self.target_net.load_state_dict(checkpoint['target_net'])\n",
        "                self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "                self.scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "                self.epsilon = checkpoint['epsilon']\n",
        "                self.steps_done = checkpoint['steps_done']\n",
        "                self.rewards_history = checkpoint['rewards_history']\n",
        "                self.loss_history = checkpoint['loss_history']\n",
        "                logger.info(f\"Model loaded from {self.model_path}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading model: {str(e)}\")\n",
        "\n",
        "    def plot_learning_curves(self):\n",
        "        \"\"\"Plot learning curves for visualization\"\"\"\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        # Plot rewards\n",
        "        plt.subplot(2, 1, 1)\n",
        "        rewards = pd.Series(self.rewards_history).rolling(100).mean()\n",
        "        plt.plot(rewards)\n",
        "        plt.title('Average Reward (100-episode rolling mean)')\n",
        "        plt.xlabel('Episodes')\n",
        "        plt.ylabel('Reward')\n",
        "\n",
        "        # Plot loss\n",
        "        plt.subplot(2, 1, 2)\n",
        "        loss = pd.Series(self.loss_history).rolling(100).mean()\n",
        "        plt.plot(loss)\n",
        "        plt.title('Average Loss (100-episode rolling mean)')\n",
        "        plt.xlabel('Training Steps')\n",
        "        plt.ylabel('Loss')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('learning_curves.png')\n",
        "        plt.close()\n",
        "\n",
        "# =============== 3. LEARNING ANALYTICS & STUDENT PROFILING ===============\n",
        "\n",
        "class LearnerProfile:\n",
        "    \"\"\"Student profiling and analytics system\"\"\"\n",
        "\n",
        "    def __init__(self, user_id=\"default_user\"):\n",
        "        self.user_id = user_id\n",
        "        self.data_file = f\"user_profiles/{user_id}.json\"\n",
        "        self.current_session = {\n",
        "            \"start_time\": datetime.now().isoformat(),\n",
        "            \"interactions\": [],\n",
        "            \"topics\": {}\n",
        "        }\n",
        "        self.profile = self._load_profile()\n",
        "\n",
        "    def _load_profile(self):\n",
        "        \"\"\"Load existing profile or create new one\"\"\"\n",
        "        try:\n",
        "            if os.path.exists(self.data_file):\n",
        "                with open(self.data_file, 'r') as f:\n",
        "                    return json.load(f)\n",
        "            else:\n",
        "                # Create directory if it doesn't exist\n",
        "                os.makedirs(os.path.dirname(self.data_file), exist_ok=True)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading profile: {str(e)}\")\n",
        "\n",
        "        # Default profile\n",
        "        return {\n",
        "            \"user_id\": self.user_id,\n",
        "            \"created_at\": datetime.now().isoformat(),\n",
        "            \"learning_preferences\": {\n",
        "                \"preferred_style\": None,\n",
        "                \"optimal_difficulty\": None,\n",
        "                \"topic_interests\": {},\n",
        "                \"engagement_patterns\": {\n",
        "                    \"average_session_duration\": 0,\n",
        "                    \"peak_activity_times\": []\n",
        "                }\n",
        "            },\n",
        "            \"performance_metrics\": {\n",
        "                \"topics\": {},\n",
        "                \"overall\": {\n",
        "                    \"correct_answers\": 0,\n",
        "                    \"incorrect_answers\": 0,\n",
        "                    \"total_attempts\": 0\n",
        "                }\n",
        "            },\n",
        "            \"sessions\": []\n",
        "        }\n",
        "\n",
        "    def record_interaction(self, interaction_data):\n",
        "        \"\"\"Record student interaction\"\"\"\n",
        "        # Add timestamp\n",
        "        interaction_data[\"timestamp\"] = datetime.now().isoformat()\n",
        "        self.current_session[\"interactions\"].append(interaction_data)\n",
        "\n",
        "        # Update topic tracking\n",
        "        topic = interaction_data.get(\"topic\", \"unknown\")\n",
        "        if topic not in self.current_session[\"topics\"]:\n",
        "            self.current_session[\"topics\"][topic] = {\n",
        "                \"attempts\": 0,\n",
        "                \"correct\": 0\n",
        "            }\n",
        "\n",
        "        self.current_session[\"topics\"][topic][\"attempts\"] += 1\n",
        "        if interaction_data.get(\"is_correct\", False):\n",
        "            self.current_session[\"topics\"][topic][\"correct\"] += 1\n",
        "\n",
        "        # Update overall metrics\n",
        "        if \"performance_metrics\" not in self.profile:\n",
        "            self.profile[\"performance_metrics\"] = {\"topics\": {}, \"overall\": {\"correct_answers\": 0, \"incorrect_answers\": 0, \"total_attempts\": 0}}\n",
        "\n",
        "        self.profile[\"performance_metrics\"][\"overall\"][\"total_attempts\"] += 1\n",
        "        if interaction_data.get(\"is_correct\", False):\n",
        "            self.profile[\"performance_metrics\"][\"overall\"][\"correct_answers\"] += 1\n",
        "        else:\n",
        "            self.profile[\"performance_metrics\"][\"overall\"][\"incorrect_answers\"] += 1\n",
        "\n",
        "        # Update topic-specific metrics\n",
        "        if topic not in self.profile[\"performance_metrics\"][\"topics\"]:\n",
        "            self.profile[\"performance_metrics\"][\"topics\"][topic] = {\n",
        "                \"attempts\": 0,\n",
        "                \"correct\": 0,\n",
        "                \"mastery_level\": 0.0\n",
        "            }\n",
        "\n",
        "        self.profile[\"performance_metrics\"][\"topics\"][topic][\"attempts\"] += 1\n",
        "        if interaction_data.get(\"is_correct\", False):\n",
        "            self.profile[\"performance_metrics\"][\"topics\"][topic][\"correct\"] += 1\n",
        "\n",
        "        # Calculate mastery level (simple version)\n",
        "        attempts = self.profile[\"performance_metrics\"][\"topics\"][topic][\"attempts\"]\n",
        "        correct = self.profile[\"performance_metrics\"][\"topics\"][topic][\"correct\"]\n",
        "        mastery = correct / max(attempts, 1) * 100\n",
        "        self.profile[\"performance_metrics\"][\"topics\"][topic][\"mastery_level\"] = mastery\n",
        "\n",
        "        # Save profile\n",
        "        self._save_profile()\n",
        "\n",
        "    def end_session(self):\n",
        "        \"\"\"End current session and save data\"\"\"\n",
        "        # Calculate session duration\n",
        "        start_time = datetime.fromisoformat(self.current_session[\"start_time\"])\n",
        "        end_time = datetime.now()\n",
        "        duration_seconds = (end_time - start_time).total_seconds()\n",
        "        self.current_session[\"duration_seconds\"] = duration_seconds\n",
        "        self.current_session[\"end_time\"] = end_time.isoformat()\n",
        "\n",
        "        # Add session to profile\n",
        "        if \"sessions\" not in self.profile:\n",
        "            self.profile[\"sessions\"] = []\n",
        "        self.profile[\"sessions\"].append(self.current_session)\n",
        "\n",
        "        # Update engagement patterns\n",
        "        if \"learning_preferences\" not in self.profile:\n",
        "            self.profile[\"learning_preferences\"] = {\n",
        "                \"preferred_style\": None,\n",
        "                \"optimal_difficulty\": None,\n",
        "                \"topic_interests\": {},\n",
        "                \"engagement_patterns\": {\n",
        "                    \"average_session_duration\": 0,\n",
        "                    \"peak_activity_times\": []\n",
        "                }\n",
        "            }\n",
        "\n",
        "        # Update average session duration\n",
        "        session_durations = [session.get(\"duration_seconds\", 0) for session in self.profile[\"sessions\"]]\n",
        "        self.profile[\"learning_preferences\"][\"engagement_patterns\"][\"average_session_duration\"] = sum(session_durations) / max(len(session_durations), 1)\n",
        "\n",
        "        # Update peak activity times\n",
        "        hour_of_day = start_time.hour\n",
        "        if hour_of_day not in self.profile[\"learning_preferences\"][\"engagement_patterns\"][\"peak_activity_times\"]:\n",
        "            self.profile[\"learning_preferences\"][\"engagement_patterns\"][\"peak_activity_times\"].append(hour_of_day)\n",
        "\n",
        "        # Update topic interests\n",
        "        for topic, data in self.current_session[\"topics\"].items():\n",
        "            if \"topic_interests\" not in self.profile[\"learning_preferences\"]:\n",
        "                self.profile[\"learning_preferences\"][\"topic_interests\"] = {}\n",
        "\n",
        "            if topic not in self.profile[\"learning_preferences\"][\"topic_interests\"]:\n",
        "                self.profile[\"learning_preferences\"][\"topic_interests\"][topic] = 0\n",
        "\n",
        "            self.profile[\"learning_preferences\"][\"topic_interests\"][topic] += data[\"attempts\"]\n",
        "\n",
        "        # Analyze learning style preference\n",
        "        self._analyze_learning_preferences()\n",
        "\n",
        "        # Save profile\n",
        "        self._save_profile()\n",
        "\n",
        "        # Reset current session\n",
        "        self.current_session = {\n",
        "            \"start_time\": datetime.now().isoformat(),\n",
        "            \"interactions\": [],\n",
        "            \"topics\": {}\n",
        "        }\n",
        "\n",
        "    def _analyze_learning_preferences(self):\n",
        "        \"\"\"Analyze student data to determine learning preferences\"\"\"\n",
        "        # Count style effectiveness\n",
        "        style_performance = {}\n",
        "        difficulty_performance = {}\n",
        "\n",
        "        for session in self.profile[\"sessions\"]:\n",
        "            for interaction in session[\"interactions\"]:\n",
        "                style = interaction.get(\"learning_style\")\n",
        "                difficulty = interaction.get(\"difficulty\")\n",
        "                is_correct = interaction.get(\"is_correct\", False)\n",
        "                if style and difficulty:\n",
        "                    # Initialize counters if needed\n",
        "                    if style not in style_performance:\n",
        "                        style_performance[style] = {\"correct\": 0, \"total\": 0}\n",
        "                    if difficulty not in difficulty_performance:\n",
        "                        difficulty_performance[difficulty] = {\"correct\": 0, \"total\": 0}\n",
        "\n",
        "                    # Update counters\n",
        "                    style_performance[style][\"total\"] += 1\n",
        "                    difficulty_performance[difficulty][\"total\"] += 1\n",
        "\n",
        "                    if is_correct:\n",
        "                        style_performance[style][\"correct\"] += 1\n",
        "                        difficulty_performance[difficulty][\"correct\"] += 1\n",
        "\n",
        "        # Determine preferred style\n",
        "        best_style = None\n",
        "        best_style_ratio = 0\n",
        "\n",
        "        for style, data in style_performance.items():\n",
        "            if data[\"total\"] >= 5:  # Minimum threshold for confident assessment\n",
        "                ratio = data[\"correct\"] / data[\"total\"]\n",
        "                if ratio > best_style_ratio:\n",
        "                    best_style_ratio = ratio\n",
        "                    best_style = style\n",
        "\n",
        "        # Determine optimal difficulty\n",
        "        best_difficulty = None\n",
        "        best_difficulty_ratio = 0\n",
        "\n",
        "        for difficulty, data in difficulty_performance.items():\n",
        "            if data[\"total\"] >= 5:  # Minimum threshold for confident assessment\n",
        "                ratio = data[\"correct\"] / data[\"total\"]\n",
        "                # Ideal ratio around 0.7-0.8 (challenging but achievable)\n",
        "                adjusted_ratio = 1 - abs(0.75 - ratio)\n",
        "                if adjusted_ratio > best_difficulty_ratio:\n",
        "                    best_difficulty_ratio = adjusted_ratio\n",
        "                    best_difficulty = difficulty\n",
        "\n",
        "        # Update profile\n",
        "        if best_style:\n",
        "            self.profile[\"learning_preferences\"][\"preferred_style\"] = best_style\n",
        "\n",
        "        if best_difficulty:\n",
        "            self.profile[\"learning_preferences\"][\"optimal_difficulty\"] = best_difficulty\n",
        "\n",
        "    def _save_profile(self):\n",
        "        \"\"\"Save user profile to file\"\"\"\n",
        "        try:\n",
        "            with open(self.data_file, 'w') as f:\n",
        "                json.dump(self.profile, f, indent=2)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error saving profile: {str(e)}\")\n",
        "\n",
        "    def get_learning_insights(self):\n",
        "        \"\"\"Generate insights from learner profile\"\"\"\n",
        "        insights = {\n",
        "            \"strengths\": [],\n",
        "            \"areas_for_improvement\": [],\n",
        "            \"recommendations\": [],\n",
        "            \"learning_style\": self.profile[\"learning_preferences\"][\"preferred_style\"],\n",
        "            \"optimal_difficulty\": self.profile[\"learning_preferences\"][\"optimal_difficulty\"]\n",
        "        }\n",
        "\n",
        "        # Find strengths (topics with high mastery)\n",
        "        for topic, data in self.profile[\"performance_metrics\"][\"topics\"].items():\n",
        "            if data[\"attempts\"] >= 3:  # Only consider topics with enough attempts\n",
        "                if data[\"mastery_level\"] >= 80:\n",
        "                    insights[\"strengths\"].append({\n",
        "                        \"topic\": topic,\n",
        "                        \"mastery\": data[\"mastery_level\"]\n",
        "                    })\n",
        "                elif data[\"mastery_level\"] <= 50:\n",
        "                    insights[\"areas_for_improvement\"].append({\n",
        "                        \"topic\": topic,\n",
        "                        \"mastery\": data[\"mastery_level\"]\n",
        "                    })\n",
        "\n",
        "        # Sort by mastery level\n",
        "        insights[\"strengths\"].sort(key=lambda x: x[\"mastery\"], reverse=True)\n",
        "        insights[\"areas_for_improvement\"].sort(key=lambda x: x[\"mastery\"])\n",
        "\n",
        "        # Limit to top 3\n",
        "        insights[\"strengths\"] = insights[\"strengths\"][:3]\n",
        "        insights[\"areas_for_improvement\"] = insights[\"areas_for_improvement\"][:3]\n",
        "\n",
        "        # Generate recommendations\n",
        "        if insights[\"areas_for_improvement\"]:\n",
        "            for area in insights[\"areas_for_improvement\"]:\n",
        "                insights[\"recommendations\"].append({\n",
        "                    \"topic\": area[\"topic\"],\n",
        "                    \"suggestion\": f\"Review {area['topic']} with {insights['learning_style'] or 'multimodal'} resources at {insights['optimal_difficulty'] or 'intermediate'} level\"\n",
        "                })\n",
        "\n",
        "        # Add general recommendation if no specific areas found\n",
        "        if not insights[\"recommendations\"]:\n",
        "            top_interests = sorted(\n",
        "                self.profile[\"learning_preferences\"][\"topic_interests\"].items(),\n",
        "                key=lambda x: x[1],\n",
        "                reverse=True\n",
        "            )[:3]\n",
        "\n",
        "            for topic, _ in top_interests:\n",
        "                insights[\"recommendations\"].append({\n",
        "                    \"topic\": topic,\n",
        "                    \"suggestion\": f\"Explore advanced concepts in {topic} to deepen understanding\"\n",
        "                })\n",
        "\n",
        "        return insights\n",
        "\n",
        "    def generate_progress_report(self):\n",
        "        \"\"\"Generate a comprehensive progress report\"\"\"\n",
        "        # Overall statistics\n",
        "        overall = self.profile[\"performance_metrics\"][\"overall\"]\n",
        "        total_attempts = overall[\"total_attempts\"]\n",
        "        accuracy = (overall[\"correct_answers\"] / total_attempts * 100) if total_attempts > 0 else 0\n",
        "\n",
        "        # Session statistics\n",
        "        num_sessions = len(self.profile[\"sessions\"])\n",
        "        avg_duration = self.profile[\"learning_preferences\"][\"engagement_patterns\"][\"average_session_duration\"] / 60  # in minutes\n",
        "\n",
        "        # Topic mastery\n",
        "        topic_mastery = []\n",
        "        for topic, data in self.profile[\"performance_metrics\"][\"topics\"].items():\n",
        "            if data[\"attempts\"] >= 3:  # Only consider topics with enough attempts\n",
        "                topic_mastery.append({\n",
        "                    \"topic\": topic,\n",
        "                    \"mastery\": data[\"mastery_level\"],\n",
        "                    \"attempts\": data[\"attempts\"]\n",
        "                })\n",
        "\n",
        "        # Sort by mastery\n",
        "        topic_mastery.sort(key=lambda x: x[\"mastery\"], reverse=True)\n",
        "\n",
        "        # Get insights\n",
        "        insights = self.get_learning_insights()\n",
        "\n",
        "        return {\n",
        "            \"summary\": {\n",
        "                \"total_sessions\": num_sessions,\n",
        "                \"total_questions_attempted\": total_attempts,\n",
        "                \"overall_accuracy\": accuracy,\n",
        "                \"average_session_duration\": avg_duration\n",
        "            },\n",
        "            \"topic_mastery\": topic_mastery,\n",
        "            \"learning_preferences\": {\n",
        "                \"preferred_style\": self.profile[\"learning_preferences\"][\"preferred_style\"],\n",
        "                \"optimal_difficulty\": self.profile[\"learning_preferences\"][\"optimal_difficulty\"]\n",
        "            },\n",
        "            \"insights\": insights\n",
        "        }\n",
        "\n",
        "# =============== 4. COMPREHENSIVE EDUCATIONAL SYSTEM INTEGRATION ===============\n",
        "\n",
        "class LearningCompanion:\n",
        "    \"\"\"Main learning companion system integrating all components\"\"\"\n",
        "\n",
        "    def __init__(self, user_id=\"default_user\"):\n",
        "        # Initialize components\n",
        "        self.content_generator = ContentGenerator()\n",
        "        self.state = LearningState()\n",
        "\n",
        "        # Initialize learner profile\n",
        "        self.learner_profile = LearnerProfile(user_id)\n",
        "\n",
        "        # Initialize RL agent (13 state features, 9 actions)\n",
        "        self.agent = AdaptiveLearningAgent(13, 9)\n",
        "\n",
        "        # Current topic and content\n",
        "        self.current_topic = None\n",
        "        self.current_content = None\n",
        "        self.last_action_time = datetime.now()\n",
        "\n",
        "        logger.info(\"Learning Companion initialized\")\n",
        "\n",
        "    def generate_content(self, topic, content_type=\"full\"):\n",
        "        \"\"\"Generate content for the specified topic\"\"\"\n",
        "        if not topic:\n",
        "            return \"Please specify a topic\"\n",
        "\n",
        "        # Record topic\n",
        "        self.current_topic = topic\n",
        "\n",
        "        # Get current difficulty and learning style\n",
        "        difficulty = self.state.get_current_difficulty()\n",
        "        learning_style = self.state.get_current_style()\n",
        "\n",
        "        # Generate content\n",
        "        content = self.content_generator.generate_content(\n",
        "            topic,\n",
        "            difficulty,\n",
        "            learning_style,\n",
        "            content_type\n",
        "        )\n",
        "\n",
        "        # Store current content\n",
        "        self.current_content = content\n",
        "\n",
        "        # Log\n",
        "        logger.info(f\"Generated {content_type} content for '{topic}' at {difficulty} level with {learning_style} style\")\n",
        "\n",
        "        return content\n",
        "\n",
        "    def evaluate_answer(self, user_answer, is_correct=None):\n",
        "        \"\"\"Evaluate user answer and update learning state\"\"\"\n",
        "        if self.current_topic is None:\n",
        "            return \"No current topic. Please generate content first.\"\n",
        "\n",
        "        # Calculate response time\n",
        "        now = datetime.now()\n",
        "        response_time = (now - self.last_action_time).total_seconds()\n",
        "        self.last_action_time = now\n",
        "\n",
        "        # If correct flag not provided, assume it's manually marked\n",
        "        if is_correct is None:\n",
        "            return \"Please mark the answer as correct or incorrect\"\n",
        "\n",
        "        # Current state (before update)\n",
        "        current_state_tensor = self.state.get_state_tensor()\n",
        "\n",
        "        # Update state\n",
        "        self.state.update(is_correct, response_time, self.current_topic)\n",
        "\n",
        "        # Record interaction in learner profile\n",
        "        self.learner_profile.record_interaction({\n",
        "            \"topic\": self.current_topic,\n",
        "            \"is_correct\": is_correct,\n",
        "            \"response_time\": response_time,\n",
        "            \"difficulty\": self.state.get_current_difficulty(),\n",
        "            \"learning_style\": self.state.get_current_style()\n",
        "        })\n",
        "\n",
        "        # Calculate reward\n",
        "        reward = self.agent.calculate_reward(\n",
        "            is_correct,\n",
        "            self.state.features['consecutive_correct'],\n",
        "            self.state.features['difficulty_level']\n",
        "        )\n",
        "\n",
        "        # Select action based on updated state\n",
        "        next_state_tensor = self.state.get_state_tensor()\n",
        "        action = self.agent.select_action(next_state_tensor)\n",
        "\n",
        "        # Store transition\n",
        "        self.agent.store_transition(\n",
        "            current_state_tensor,\n",
        "            action,\n",
        "            reward,\n",
        "            next_state_tensor,\n",
        "            False  # Not a terminal state\n",
        "        )\n",
        "\n",
        "        # Apply action to learning state\n",
        "        self.state = self.agent.apply_action(action, self.state)\n",
        "\n",
        "        # Optimize model\n",
        "        loss = self.agent.optimize_model()\n",
        "\n",
        "        # Prepare response\n",
        "        if is_correct:\n",
        "            feedback = f\"Correct! Your answer demonstrates understanding of {self.current_topic}.\"\n",
        "        else:\n",
        "            feedback = f\"Not quite right. Let's review {self.current_topic} again.\"\n",
        "\n",
        "        # Add adaptation info\n",
        "        adaptation = f\"\\n\\nAdjusting to your learning: Now using {self.state.get_current_style()} style at {self.state.get_current_difficulty()} level.\"\n",
        "\n",
        "        # Save agent model periodically\n",
        "        if self.agent.steps_done % 50 == 0:\n",
        "            self.agent.save_model()\n",
        "\n",
        "        return feedback + adaptation\n",
        "\n",
        "    def get_insights(self):\n",
        "        \"\"\"Get learning insights for the student\"\"\"\n",
        "        return self.learner_profile.get_learning_insights()\n",
        "\n",
        "    def get_progress_report(self):\n",
        "        \"\"\"Generate comprehensive progress report\"\"\"\n",
        "        return self.learner_profile.generate_progress_report()\n",
        "\n",
        "    def end_session(self):\n",
        "        \"\"\"End the current learning session\"\"\"\n",
        "        self.learner_profile.end_session()\n",
        "        self.agent.save_model()\n",
        "        return \"Session ended and progress saved.\"\n",
        "\n",
        "    def visualize_learning(self):\n",
        "        \"\"\"Generate visualizations of learning progress\"\"\"\n",
        "        self.agent.plot_learning_curves()\n",
        "        return \"Learning curves generated and saved to 'learning_curves.png'\"\n",
        "\n",
        "# =============== 5. GRADIO INTERFACE ===============\n",
        "\n",
        "def create_interface():\n",
        "    \"\"\"Create Gradio interface for the Learning Companion\"\"\"\n",
        "\n",
        "    # Initialize learning companion\n",
        "    companion = LearningCompanion()\n",
        "\n",
        "    # Define Gradio components and functions\n",
        "    def generate_content_fn(topic, content_type):\n",
        "        return companion.generate_content(topic, content_type)\n",
        "\n",
        "    def evaluate_answer_fn(user_answer, is_correct):\n",
        "        return companion.evaluate_answer(user_answer, is_correct == \"Correct\")\n",
        "\n",
        "    def get_insights_fn():\n",
        "        insights = companion.get_insights()\n",
        "\n",
        "        # Format insights for display\n",
        "        result = \"## Learning Insights\\n\\n\"\n",
        "\n",
        "        result += \"### Strengths\\n\"\n",
        "        for strength in insights[\"strengths\"]:\n",
        "            result += f\"- {strength['topic']}: {strength['mastery']:.1f}% mastery\\n\"\n",
        "\n",
        "        result += \"\\n### Areas for Improvement\\n\"\n",
        "        for area in insights[\"areas_for_improvement\"]:\n",
        "            result += f\"- {area['topic']}: {area['mastery']:.1f}% mastery\\n\"\n",
        "\n",
        "        result += \"\\n### Recommendations\\n\"\n",
        "        for rec in insights[\"recommendations\"]:\n",
        "            result += f\"- {rec['suggestion']}\\n\"\n",
        "\n",
        "        result += f\"\\n### Learning Preferences\\n\"\n",
        "        result += f\"- Preferred Learning Style: {insights['learning_style'] or 'Not enough data'}\\n\"\n",
        "        result += f\"- Optimal Difficulty Level: {insights['optimal_difficulty'] or 'Not enough data'}\\n\"\n",
        "\n",
        "        return result\n",
        "\n",
        "    def get_progress_report_fn():\n",
        "        report = companion.get_progress_report()\n",
        "\n",
        "        # Format report for display\n",
        "        result = \"# Learning Progress Report\\n\\n\"\n",
        "\n",
        "        # Summary\n",
        "        summary = report[\"summary\"]\n",
        "        result += \"## Summary\\n\"\n",
        "        result += f\"- Total Sessions: {summary['total_sessions']}\\n\"\n",
        "        result += f\"- Questions Attempted: {summary['total_questions_attempted']}\\n\"\n",
        "        result += f\"- Overall Accuracy: {summary['overall_accuracy']:.1f}%\\n\"\n",
        "        result += f\"- Average Session Duration: {summary['average_session_duration']:.1f} minutes\\n\\n\"\n",
        "\n",
        "        # Topic Mastery\n",
        "        result += \"## Topic Mastery\\n\"\n",
        "        for topic in report[\"topic_mastery\"]:\n",
        "            result += f\"- {topic['topic']}: {topic['mastery']:.1f}% ({topic['attempts']} attempts)\\n\"\n",
        "\n",
        "        # Learning Preferences\n",
        "        prefs = report[\"learning_preferences\"]\n",
        "        result += \"\\n## Learning Preferences\\n\"\n",
        "        result += f\"- Preferred Learning Style: {prefs['preferred_style'] or 'Not enough data'}\\n\"\n",
        "        result += f\"- Optimal Difficulty Level: {prefs['optimal_difficulty'] or 'Not enough data'}\\n\"\n",
        "\n",
        "        # Insights\n",
        "        insights = report[\"insights\"]\n",
        "        result += \"\\n## Insights and Recommendations\\n\"\n",
        "        for rec in insights[\"recommendations\"]:\n",
        "            result += f\"- {rec['suggestion']}\\n\"\n",
        "\n",
        "        return result\n",
        "\n",
        "    def end_session_fn():\n",
        "        return companion.end_session()\n",
        "\n",
        "    def visualize_learning_fn():\n",
        "        companion.visualize_learning()\n",
        "        return gr.Image(value=\"learning_curves.png\")\n",
        "\n",
        "    # Create Gradio interface\n",
        "    with gr.Blocks(title=\"Adaptive Learning Companion\") as interface:\n",
        "        gr.Markdown(\"# 🧠 Adaptive Learning Companion\")\n",
        "        gr.Markdown(\"An AI-powered personalized learning system with automatic adaptation to your learning style and performance.\")\n",
        "\n",
        "        with gr.Tab(\"Learn\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=3):\n",
        "                    topic_input = gr.Textbox(label=\"Topic\", placeholder=\"Enter a topic to learn (e.g., 'Python functions', 'Photosynthesis', 'Linear equations')\")\n",
        "                    content_type = gr.Radio(\n",
        "                        [\"full\", \"lesson\", \"quiz\"],\n",
        "                        label=\"Content Type\",\n",
        "                        value=\"full\"\n",
        "                    )\n",
        "                    generate_btn = gr.Button(\"Generate Learning Content\")\n",
        "\n",
        "                with gr.Column(scale=7):\n",
        "                    content_output = gr.Markdown(label=\"Learning Content\")\n",
        "\n",
        "            with gr.Row():\n",
        "                user_answer = gr.Textbox(label=\"Your Answer\", placeholder=\"Type your answer here...\")\n",
        "                evaluation = gr.Radio([\"Correct\", \"Incorrect\"], label=\"Mark Answer As\")\n",
        "                evaluate_btn = gr.Button(\"Submit and Evaluate\")\n",
        "\n",
        "            feedback_output = gr.Markdown(label=\"Feedback\")\n",
        "\n",
        "        with gr.Tab(\"Progress & Insights\"):\n",
        "            with gr.Row():\n",
        "                insights_btn = gr.Button(\"Get Learning Insights\")\n",
        "                report_btn = gr.Button(\"Generate Full Progress Report\")\n",
        "\n",
        "            with gr.Row():\n",
        "                insights_output = gr.Markdown(label=\"Learning Insights\")\n",
        "\n",
        "            with gr.Row():\n",
        "                report_output = gr.Markdown(label=\"Progress Report\")\n",
        "\n",
        "            with gr.Row():\n",
        "                visualize_btn = gr.Button(\"Visualize Learning\")\n",
        "                visualization_output = gr.Image(label=\"Learning Curves\")\n",
        "\n",
        "        with gr.Tab(\"Session Management\"):\n",
        "            with gr.Row():\n",
        "                end_session_btn = gr.Button(\"End Current Session\")\n",
        "                session_output = gr.Textbox(label=\"Session Status\")\n",
        "\n",
        "        # Set up event handlers\n",
        "        generate_btn.click(\n",
        "            generate_content_fn,\n",
        "            inputs=[topic_input, content_type],\n",
        "            outputs=content_output\n",
        "        )\n",
        "\n",
        "        evaluate_btn.click(\n",
        "            evaluate_answer_fn,\n",
        "            inputs=[user_answer, evaluation],\n",
        "            outputs=feedback_output\n",
        "        )\n",
        "\n",
        "        insights_btn.click(\n",
        "            get_insights_fn,\n",
        "            inputs=None,\n",
        "            outputs=insights_output\n",
        "        )\n",
        "\n",
        "        report_btn.click(\n",
        "            get_progress_report_fn,\n",
        "            inputs=None,\n",
        "            outputs=report_output\n",
        "        )\n",
        "\n",
        "        visualize_btn.click(\n",
        "            visualize_learning_fn,\n",
        "            inputs=None,\n",
        "            outputs=visualization_output\n",
        "        )\n",
        "\n",
        "        end_session_btn.click(\n",
        "            end_session_fn,\n",
        "            inputs=None,\n",
        "            outputs=session_output\n",
        "        )\n",
        "\n",
        "    return interface\n",
        "\n",
        "# =============== 6. APPLICATION ENTRY POINT ===============\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Create and launch interface\n",
        "    interface = create_interface()\n",
        "    interface.launch(share=True)\n",
        "    logger.info(\"Learning Companion launched\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "PHKBwuMSxfkr",
        "outputId": "e4798f01-12e8-4dd1-f455-dcc36a00cffd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://96ef582362979be4f2.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://96ef582362979be4f2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}